{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow E2E MNIST Case: Building, Distributed Training and Serving\n",
    "\n",
    "This example guides you through:\n",
    "  1. Taking an example TensorFlow model and modifying it to support distributed training.\n",
    "  1. Using `Kubeflow Fairing` to build docker image and launch a TFJob to train model.\n",
    "  1. Using `Kubeflow Fairing` to create InferenceService (KFServing) for the trained model.\n",
    "  1. Clean up the TFJob and InferenceService using `kubeflow-tfjob` and `kfserving` SDK client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "  * The Kubeflow Fairing, TF-Operator and KFServing have been installed in Kubenertes Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Code\n",
    "\n",
    "We modified the [examples](https://github.com/tensorflow/tensorflow/blob/9a24e8acfcd8c9046e1abaac9dbf5e146186f4c2/tensorflow/examples/learn/mnist.py) to be better suited for distributed training and model serving. There is a delta between existing distributed mnist examples and what's needed to run well as a TFJob. The updated training code is [mnist.py](mnist.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kubeflow-fairing\r\n",
      "Version: 1.0.2\r\n",
      "Summary: Kubeflow Fairing Python SDK.\r\n",
      "Home-page: https://github.com/kubeflow/fairing\r\n",
      "Author: Kubeflow Authors\r\n",
      "Author-email: hejinchi@cn.ibm.com\r\n",
      "License: Apache License Version 2.0\r\n",
      "Location: /home/jupyter/anaconda3/envs/mlpipeline/lib/python3.7/site-packages\r\n",
      "Requires: six, urllib3, grpcio, setuptools, notebook, oauth2client, cloudpickle, kubeflow-pytorchjob, kubernetes, httplib2, kubeflow-tfjob, azure-mgmt-storage, python-dateutil, ibm-cos-sdk, tornado, future, kfserving, retrying, google-cloud-logging, google-cloud-storage, numpy, nbconvert, google-auth, requests, azure-storage-file, google-api-python-client, boto3, docker\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show kubeflow-fairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Docker Registry for Kubeflow Fairing\n",
    "\n",
    "* In order to build docker images from your notebook we need a docker registry where the images will be stored\n",
    "\n",
    "**Note:** The below section must be updated to your values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist\n"
     ]
    }
   ],
   "source": [
    "# Set docker registry to store image.\n",
    "# Ensure you have permission for pushing docker image requests. \n",
    "DOCKER_REGISTRY = 'index.docker.io/yiluxiangbei'\n",
    "\n",
    "# Set namespace. Note that the created PVC should be in the namespace.\n",
    "my_namespace = 'mnist'\n",
    "# You also can get the default target namepspace using below API.\n",
    "#namespace = fairing_utils.get_default_target_namespace()\n",
    "print(my_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PV/PVC to Store the Exported Model \n",
    "\n",
    "Create Persistent Volume(PV) and Persistent Volume Claim(PVC), the PVC will be used by pods of training and serving for local mode in steps below.\n",
    "\n",
    "**Note:** The below section must be updated to your values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To satify the distributed training, the PVC should be access from all nodes in the cluster.\n",
    "# The example creates a NFS PV to satify that.\n",
    "nfs_server = '172.17.56.182'\n",
    "nfs_path = '/root/nfs_root/'\n",
    "pv_name = 'mnist-e2e-pv'\n",
    "pvc_name = 'mnist-e2e-pvc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Skip below creating PV/PVC step if you set an existing PV and PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in k8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'api_version': 'v1',\n",
       " 'kind': 'PersistentVolumeClaim',\n",
       " 'metadata': {'annotations': None,\n",
       "              'cluster_name': None,\n",
       "              'creation_timestamp': datetime.datetime(2020, 9, 2, 10, 42, 46, tzinfo=tzutc()),\n",
       "              'deletion_grace_period_seconds': None,\n",
       "              'deletion_timestamp': None,\n",
       "              'finalizers': ['kubernetes.io/pvc-protection'],\n",
       "              'generate_name': None,\n",
       "              'generation': None,\n",
       "              'initializers': None,\n",
       "              'labels': None,\n",
       "              'managed_fields': [{'api_version': 'v1',\n",
       "                                  'fields': None,\n",
       "                                  'manager': 'Swagger-Codegen',\n",
       "                                  'operation': 'Update',\n",
       "                                  'time': datetime.datetime(2020, 9, 2, 10, 42, 46, tzinfo=tzutc())}],\n",
       "              'name': 'mnist-e2e-pvc',\n",
       "              'namespace': 'mnist',\n",
       "              'owner_references': None,\n",
       "              'resource_version': '5220521',\n",
       "              'self_link': '/api/v1/namespaces/mnist/persistentvolumeclaims/mnist-e2e-pvc',\n",
       "              'uid': 'a1947311-d7fa-4b7a-8a6d-b990bc4f3ba4'},\n",
       " 'spec': {'access_modes': ['ReadWriteMany'],\n",
       "          'data_source': None,\n",
       "          'resources': {'limits': None, 'requests': {'storage': '10Gi'}},\n",
       "          'selector': None,\n",
       "          'storage_class_name': '',\n",
       "          'volume_mode': 'Filesystem',\n",
       "          'volume_name': None},\n",
       " 'status': {'access_modes': None,\n",
       "            'capacity': None,\n",
       "            'conditions': None,\n",
       "            'phase': 'Pending'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kubernetes import client as k8s_client\n",
    "from kubernetes import config as k8s_config\n",
    "from kubeflow.fairing.utils import is_running_in_k8s\n",
    "import yaml\n",
    "\n",
    "pv_yaml = f'''\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: {pv_name}\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 10Gi\n",
    "  accessModes:\n",
    "  - ReadWriteMany\n",
    "  persistentVolumeReclaimPolicy: Retain\n",
    "  nfs:\n",
    "    path: {nfs_path}\n",
    "    server: {nfs_server}\n",
    "'''\n",
    "pvc_yaml = f'''\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: {pvc_name}\n",
    "  namespace: {my_namespace}\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  storageClassName: \"\"\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Gi\n",
    "'''\n",
    "\n",
    "if is_running_in_k8s():\n",
    "    print('in k8s')\n",
    "    k8s_config.load_incluster_config()\n",
    "else:\n",
    "    print('not in k8s')\n",
    "    # vim ~/.kube/config\n",
    "    k8s_config.load_kube_config()\n",
    "\n",
    "k8s_core_api = k8s_client.CoreV1Api()\n",
    "k8s_core_api.create_persistent_volume(yaml.safe_load(pv_yaml))\n",
    "k8s_core_api.create_namespaced_persistent_volume_claim(my_namespace, yaml.safe_load(pvc_yaml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Kubeflow fairing to build the docker image and launch a TFJob for training\n",
    "\n",
    "* Use kubeflow fairing to build a docker image that includes all your dependencies\n",
    "* Launch a TFJob in the on premise cluster to taining model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly set some custom training parameters for TFJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chief = 1 #number of Chief in TFJob \n",
    "num_ps = 1  #number of PS in TFJob \n",
    "num_workers = 2  #number of Worker in TFJob \n",
    "model_dir = \"/mnt\"\n",
    "export_path = \"/mnt/export\" \n",
    "train_steps = \"1000\"\n",
    "batch_size = \"100\"\n",
    "learning_rate = \"0.01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Kubeflow Fairing to build a docker image and push to docker registry, and then launch a TFJob in the on-prem cluster for distributed training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 200902 18:46:59 utils:51] The function mounting_pvc has been deprecated,                     please use `volume_mounts`\n",
      "[I 200902 18:46:59 config:134] Using preprocessor: <kubeflow.fairing.preprocessors.base.BasePreProcessor object at 0x7f997bb72e90>\n",
      "[I 200902 18:46:59 config:136] Using builder: <kubeflow.fairing.builders.docker.docker.DockerBuilder object at 0x7f994e9b2190>\n",
      "[I 200902 18:46:59 config:138] Using deployer: <kubeflow.fairing.deployers.tfjob.tfjob.TfJob object at 0x7f994debed10>\n",
      "[I 200902 18:46:59 docker:32] Building image using docker\n",
      "[W 200902 18:46:59 docker:41] Docker command: ['python', '/opt/mnist.py', '--tf-model-dir=/mnt', '--tf-export-dir=/mnt/export', '--tf-train-steps=1000', '--tf-batch-size=100', '--tf-learning-rate=0.01']\n",
      "[I 200902 18:46:59 base:107] Creating docker context: /tmp/fairing_context_qbnjnapm\n",
      "[W 200902 18:46:59 docker:56] Building docker image index.docker.io/yiluxiangbei/mnist:34EC2DA6...\n",
      "[I 200902 18:46:59 docker:103] Build output: Step 1/5 : FROM tensorflow/tensorflow:1.15.2-py3\n",
      "[I 200902 18:46:59 docker:103] Build output: \n",
      "[I 200902 18:47:00 docker:103] Push output: Pulling from tensorflow/tensorflow None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Already exists None\n",
      "[I 200902 18:47:00 docker:103] Push output: Digest: sha256:28b5f547969d70f825909c8fe06675ffc2959afe6079aeae754afa312f6417b9 None\n",
      "[I 200902 18:47:00 docker:103] Push output: Status: Downloaded newer image for tensorflow/tensorflow:1.15.2-py3 None\n",
      "[I 200902 18:47:00 docker:103] Build output: ---> b2b972268a17\n",
      "[I 200902 18:47:00 docker:103] Build output: Step 2/5 : ADD mnist.py /opt/mnist.py\n",
      "[I 200902 18:47:00 docker:103] Build output: \n",
      "[I 200902 18:47:01 docker:103] Build output: ---> 2b5e7aea7ded\n",
      "[I 200902 18:47:01 docker:103] Build output: Step 3/5 : RUN chmod +x /opt/mnist.py\n",
      "[I 200902 18:47:01 docker:103] Build output: \n",
      "[I 200902 18:47:01 docker:103] Build output: ---> Running in 2f8123a11f24\n",
      "[I 200902 18:47:02 docker:103] Build output: ---> 4d3de9f82c45\n",
      "[I 200902 18:47:02 docker:103] Build output: Step 4/5 : ENTRYPOINT [\"/usr/bin/python\"]\n",
      "[I 200902 18:47:02 docker:103] Build output: \n",
      "[I 200902 18:47:02 docker:103] Build output: ---> Running in ffd8d654a66d\n",
      "[I 200902 18:47:03 docker:103] Build output: ---> 2629af3cbbd5\n",
      "[I 200902 18:47:03 docker:103] Build output: Step 5/5 : CMD [\"/opt/mnist.py\"]\n",
      "[I 200902 18:47:03 docker:103] Build output: \n",
      "[I 200902 18:47:03 docker:103] Build output: ---> Running in c50a1aa06364\n",
      "[I 200902 18:47:03 docker:103] Build output: ---> c6b47b8b466e\n",
      "[I 200902 18:47:03 docker:103] Push finished: {'ID': 'sha256:c6b47b8b466e89c496fd33a106109acc52385d73add75da98257a19bfb10efde'}\n",
      "[I 200902 18:47:03 docker:103] Build output: Successfully built c6b47b8b466e\n",
      "[I 200902 18:47:03 docker:103] Build output: Successfully tagged yiluxiangbei/mnist:34EC2DA6\n",
      "[W 200902 18:47:03 docker:70] Publishing image index.docker.io/yiluxiangbei/mnist:34EC2DA6...\n",
      "[I 200902 18:47:03 docker:103] Push output: The push refers to repository [docker.io/yiluxiangbei/mnist] None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Preparing None\n",
      "[I 200902 18:47:04 docker:103] Push output: Waiting None\n",
      "[I 200902 18:47:04 docker:103] Push output: Waiting None\n",
      "[I 200902 18:47:04 docker:103] Push output: Waiting None\n",
      "[I 200902 18:47:04 docker:103] Push output: Waiting None\n",
      "[I 200902 18:47:04 docker:103] Push output: Waiting None\n",
      "[I 200902 18:47:04 docker:103] Push output: Waiting None\n",
      "[I 200902 18:47:04 docker:103] Push output: Waiting None\n",
      "[I 200902 18:47:07 docker:103] Push output: Pushing [==>                                                ]     512B/9.104kB\n",
      "[I 200902 18:47:07 docker:103] Push output: Pushing [==================================================>]  11.26kB\n",
      "[I 200902 18:47:07 docker:103] Push output: Pushing [==>                                                ]     512B/9.104kB\n",
      "[I 200902 18:47:07 docker:103] Push output: Pushing [==================================================>]  11.26kB\n",
      "[I 200902 18:47:14 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:14 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:14 docker:103] Push output: Pushed None\n",
      "[I 200902 18:47:14 docker:103] Push output: Pushed None\n",
      "[I 200902 18:47:17 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:21 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:22 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:22 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:27 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:28 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:29 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:29 docker:103] Push output: Mounted from tensorflow/tensorflow None\n",
      "[I 200902 18:47:42 docker:103] Push output: 34EC2DA6: digest: sha256:68879fe45c569b8bd5fb5f567546d063969575409c4cb9b95c3b8168463d9a6d size: 2828 None\n",
      "[I 200902 18:47:42 docker:103] Push finished: {'Tag': '34EC2DA6', 'Digest': 'sha256:68879fe45c569b8bd5fb5f567546d063969575409c4cb9b95c3b8168463d9a6d', 'Size': 2828}\n",
      "[W 200902 18:47:42 job:101] The tfjob mnist-training-ae2d launched.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<kubeflow.fairing.preprocessors.base.BasePreProcessor at 0x7f997bb72e90>,\n",
       " <kubeflow.fairing.builders.docker.docker.DockerBuilder at 0x7f994e9b2190>,\n",
       " <kubeflow.fairing.deployers.tfjob.tfjob.TfJob at 0x7f994debed10>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "from kubeflow import fairing   \n",
    "from kubeflow.fairing.kubernetes.utils import mounting_pvc\n",
    "\n",
    "tfjob_name = f'mnist-training-{uuid.uuid4().hex[:4]}'\n",
    "\n",
    "output_map =  {\n",
    "    \"Dockerfile\": \"Dockerfile\",\n",
    "    \"mnist.py\": \"mnist.py\"\n",
    "}\n",
    "\n",
    "command=[\"python\",\n",
    "         \"/opt/mnist.py\",\n",
    "         \"--tf-model-dir=\" + model_dir,\n",
    "         \"--tf-export-dir=\" + export_path,\n",
    "         \"--tf-train-steps=\" + train_steps,\n",
    "         \"--tf-batch-size=\" + batch_size,\n",
    "         \"--tf-learning-rate=\" + learning_rate]\n",
    "\n",
    "fairing.config.set_preprocessor('python', command=command, path_prefix=\"/app\", output_map=output_map)\n",
    "fairing.config.set_builder(name='docker', registry=DOCKER_REGISTRY, base_image=\"\",\n",
    "                           image_name=\"mnist\", dockerfile_path=\"Dockerfile\")\n",
    "fairing.config.set_deployer(name='tfjob', namespace=my_namespace, stream_log=False, job_name=tfjob_name,\n",
    "                            chief_count=num_chief, worker_count=num_workers, ps_count=num_ps, \n",
    "                            pod_spec_mutators=[mounting_pvc(pvc_name=pvc_name, pvc_mount_path=model_dir)])\n",
    "fairing.config.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Created TFJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception when calling CustomObjectsApi->get_namespaced_custom_object:            (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Wed, 02 Sep 2020 09:25:37 GMT', 'Content-Length': '246'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"tfjobs.kubeflow.org \\\"mnist-training-dcde\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"mnist-training-dcde\",\"group\":\"kubeflow.org\",\"kind\":\"tfjobs\"},\"code\":404}\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/kubeflow/tfjob/api/tf_job_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name, namespace, watch, timeout_seconds)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m           \u001b[0mtfjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPISERVER_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m                                      \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                      _request_timeout=_request_timeout)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                         \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                                         headers=headers)\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py\u001b[0m in \u001b[0;36mGET\u001b[0;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    230\u001b[0m                             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                             query_params=query_params)\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Wed, 02 Sep 2020 09:25:37 GMT', 'Content-Length': '246'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"tfjobs.kubeflow.org \\\"mnist-training-dcde\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"mnist-training-dcde\",\"group\":\"kubeflow.org\",\"kind\":\"tfjobs\"},\"code\":404}\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2e5b360eba89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtfjob_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFJobClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfjob_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_namespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/kubeflow/tfjob/api/tf_job_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name, namespace, watch, timeout_seconds)\u001b[0m\n\u001b[1;32m    110\u001b[0m           raise RuntimeError(\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m\"\u001b[0m\u001b[0mException\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mCustomObjectsApi\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mget_namespaced_custom_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             %s\\n\" % e)\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m           raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception when calling CustomObjectsApi->get_namespaced_custom_object:            (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Wed, 02 Sep 2020 09:25:37 GMT', 'Content-Length': '246'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"tfjobs.kubeflow.org \\\"mnist-training-dcde\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"mnist-training-dcde\",\"group\":\"kubeflow.org\",\"kind\":\"tfjobs\"},\"code\":404}\n\n\n"
     ]
    }
   ],
   "source": [
    "from kubeflow.tfjob import TFJobClient\n",
    "tfjob_client = TFJobClient()\n",
    "\n",
    "tfjob_client.get(tfjob_name, namespace=my_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait For the Training Job to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjob_client.wait_for_job(tfjob_name, namespace=my_namespace, watch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the TFJob succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjob_client.is_job_succeeded(tfjob_name, namespace=my_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjob_client.get_logs(tfjob_name, namespace=my_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Service using KFServing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.fairing.deployers.kfserving.kfserving import KFServing\n",
    "isvc_name = f'mnist-service-{uuid.uuid4().hex[:4]}'\n",
    "isvc = KFServing('tensorflow', namespace=my_namespace, isvc_name=isvc_name,\n",
    "                 default_storage_uri='pvc://' + pvc_name + '/export')\n",
    "isvc.deploy(isvc.generate_isvc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfserving import KFServingClient\n",
    "kfserving_client = KFServingClient()\n",
    "kfserving_client.get(namespace=my_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the InferenceService and Service Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_isvc = kfserving_client.get(isvc_name, namespace=my_namespace)\n",
    "mnist_isvc_name = mnist_isvc['metadata']['name']\n",
    "mnist_isvc_endpoint = mnist_isvc['status'].get('url', '')\n",
    "print(\"MNIST Service Endpoint: \" + mnist_isvc_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a prediction to the InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISTIO_CLUSTER_IP=!kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.clusterIP}'\n",
    "CLUSTER_IP=ISTIO_CLUSTER_IP[0]\n",
    "MODEL_HOST=f\"Host: {mnist_isvc_name}.{my_namespace}.example.com\"\n",
    "!curl -v -H \"{MODEL_HOST}\" http://{CLUSTER_IP}/v1/models/{mnist_isvc_name}:predict -d @./input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the TFJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjob_client.delete(tfjob_name, namespace=my_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the InferenceService."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfserving_client.delete(isvc_name, namespace=my_namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
